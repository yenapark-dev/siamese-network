# -*- coding: utf-8 -*-
"""IFN680_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Iv3Ix5N4Coln1NRWUDfT0MdHWqACS8E2

If you run this file, it automatically executes the main function.
You can repeat the experiments by setting the repeat times at the end of main functions.

STUDENT 1: HAYOUNG LEE – N10110364
STUDENT 2: YENA PARK – N10837353
STUDENT 3: DORIS (XINYANG) CHE – N10892915
"""

import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import random

from keras import layers
from keras.models import Sequential
import tensorflow_datasets as tfds

import numpy as np
import tensorflow as tf
import random
import matplotlib.pyplot as plt

class Dataset:
    '''
    To get te dataset Omniglot from tensorflow dataset.
    Split it to train and test datasets.
    '''
   
    def __init__(self, training):
        '''
          Initial function

          @training : True or False. If True, get training dataset else get test dataset
        '''
        # Set split condition. if split is True => get training set, else => get test set
        split = "train" if training else "test"
        # Download Omniglot dataset using tensorflow_dataset with dataset information
        train,ds_info = tfds.load('omniglot', split=split, with_info=True)
        
    
        # To save data
        self.data = []
        # type cast: Tensor to numpy array
        train_numpy = tfds.as_numpy(train)
        # To save all training data
        trains = []
        # To save all labels
        labels = []

        # Iterate over the dataset
        for x in train_numpy:
          # Get each image and class, and put that data into a list
            trains.append(x['image'])
            labels.append(x['label'])
        
        # type cast: list to numpy array (trains and labels )
        trains=np.array(trains)
        labels=np.array(labels)

        def extraction(trains, labels):
          '''
            reshape image using resized width and height.
            
            @trains : train dataset
            @labels : labels dataset

            return : reshaped trains, labels.
          '''

          # Get image's withd and height using trains.shape
          img_rows, img_cols = trains.shape[1:3]
          # Reshape trains
          trains = trains.reshape(trains.shape[0], img_rows, img_cols, 3)
          # Return reshape results
          return trains, labels
        
        # Excute extraction
        trains,labels = extraction(trains,labels)
        # Use zip to make a pair of image and label
        for image, label in zip(trains, labels):
          # To resize the Omniglot images to the 28*28 and divided it to 255 to make it as an image
            image = tf.image.resize(image, [28,28])/255.
            # Add image into self.data
            self.data.append(image)
        # Add label in to self.labels
        self.labels = labels

def plot_triplets(examples):
      '''
        To visualise triplets images.
        @examples : image list(length=3)
      '''
      # Set matplot figure size 6,2
      plt.figure(figsize=(6, 2))
      # Iterate 3 times => for triplet dataset
      for i in range(3):
          # Make subplot
          plt.subplot(1, 3, 1 + i)
          # Define image with reshape ((#image,28,28,3) to (28,28,3))
          plt.imshow(examples[i].reshape(28,28,3))
          # Set x ticks
          plt.xticks([])
          # Set y ticks
          plt.yticks([])
      # Show images
      plt.show()

def generate_triplet_dataset(img, label,img_size):
      '''
        Create triplet dataset : anchors, positives, negatives set
        To input data in the model.
        @img : Image array which is from training, validation or test dataset
        @label: Label array which is from training, validation or test dataset
        @img_size : The number of input dataset

        return:[anchors,positives,nagatives] => triplet dataset
      '''

      # Make zero array for anchors
      anchors = np.zeros((img_size, 28,28,3))
      # Make zero array for positives
      positives = np.zeros((img_size, 28,28,3))
      # Make zero array for negatives
      negatives = np.zeros((img_size, 28,28,3))
      
      # Iterate range 0 to img_size and make triplet data
      for i in range(0, img_size):
          # Set random_index
          random_index = random.randint(0, img.shape[0] - 1)
          # Anchor is randomly chosen
          anchor = img[random_index]
          # Find label which is for anchor
          y = label[random_index]

          # Find positive indices which are same as y
          indices_for_pos = np.squeeze(np.where(label == y))
          # Find negative indices which are different from y
          indices_for_neg = np.squeeze(np.where(label != y))
      
          # Find positive images using indices
          positive = img[indices_for_pos[random.randint(0, len(indices_for_pos) - 1)]]
          # Find negative images using indices
          negative = img[indices_for_neg[random.randint(0, len(indices_for_neg) - 1)]]
      
          # Save anchor, positive, negative image to zero array to make them have real data
          anchors[i] = anchor
          positives[i] = positive
          negatives[i] = negative

      return [anchors, positives, negatives]

def triplet_loss(y_true, y_pred, margin=0.5, emb_size = 126):
      '''
        Calculate triplet loss
        @y_true : answer label
        @y_pred: predicted label
        @margin : to decide it's positive or negative
        @emb_size :  set embedding size as 126 which is the base model's last layer's output shape

        return: loss
      '''

      # Find predicted data
      # Actually, y_pred can divided by 3 because it integrate anchor,positive,negative result.
      anchor, positive, negative = y_pred[:,:emb_size], y_pred[:,emb_size:2*emb_size], y_pred[:,2*emb_size:]
      
      # Calculate L2 distance between anchor and positive
      positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)
      # Calculate L2 distance between anchor and negative
      negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)

      # Return loss by triplet loss formula
      return tf.maximum(positive_dist - negative_dist + margin, 0.)

def data_generator(img,label,img_size,emb_size=126):
      '''
        Generate dataset to fit it for triplet inputs
        @img : Image array which is from training, validation or test dataset
        @label: Label array which is from training, validation or test dataset
        @img_size : The number of input dataset
        @emb_size :  set embedding size as 126 which is the base model's last layer's output shape
        return: x,y => x: image set, y: label set 
      '''
      # To  use infinite loop
      while True:
          # Generate triplet dataset
          x = generate_triplet_dataset(img,label,img_size)
          # Make zeor array: shape =(img_size, 3*emb_size) => input size is tripled compare to emb_size therefore, multiply 3 to emb_size
          y = np.zeros((img_size, 3*emb_size))
          return x, y

def create_base_model(input_shape):
    '''
        Create base CNN model.
        Model: "sequential"
        _________________________________________________________________
        Layer (type)                 Output Shape              Param #   
        =================================================================
        conv2d (Conv2D)              (None, 26, 26, 32)        896       
        _________________________________________________________________
        conv2d (Conv2D)            (None, 24, 24, 64)        18496     
        _________________________________________________________________
        max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         
        _________________________________________________________________
        dropout (Dropout)            (None, 12, 12, 64)        0         
        _________________________________________________________________
        flatten (Flatten)            (None, 9216)              0         
        _________________________________________________________________
        dense (Dense)                (None, 128)               1179776   
        _________________________________________________________________
        dropout (Dropout)          (None, 128)               0         
        _________________________________________________________________
        dense (Dense)              (None, 126)               16254     
        =================================================================
        @input_shape :  for input which will put in the model
        
        return: model => base model
    '''
    # Set embedding size as 126 which is the last layer's output shape
    emb_size = 126
    # Set input shape(width, height, colour)
    # input_shape = (28, 28, 3)
    # Adds layers to the sequential model
    model = Sequential()
    # Add convolution network, activation is relu, kernal size is 3*3, outputs are 32
    model.add(layers.Convolution2D(32, (3, 3), activation='relu',
                          input_shape=input_shape))
    # Add convolution network, activation is relu, kernal size is 3*3, outputs are 64
    model.add(layers.Conv2D(64, (3,3), activation='relu'))
    # Add max pooling layer, pool size is 2
    model.add(layers.MaxPooling2D(pool_size=2))
    # Add dropout layer, rate is 0.25
    model.add(layers.Dropout(0.25))
    # Add Flatten layer
    model.add(layers.Flatten())
    # Add dense layer, outputs are 128, activation function is relu
    model.add(layers.Dense(128, activation='relu'))
    # Add dropout layer, rate is 0.1
    model.add(layers.Dropout(0.1))
    # Add dense layer, outputs are emb_size which is 126, activation function is sigmoid
    model.add(layers.Dense(emb_size, activation='sigmoid'))

    # Show model summary
    model.summary()

    return model

def triplet_siamese_network(input_shape):
    '''
        Create Triplet Siamese Network
        Model: "model"
        __________________________________________________________________________________________________
        Layer (type)                    Output Shape         Param #     Connected to                     
        ==================================================================================================
        input_1 (InputLayer)            [(None, 28, 28, 3)]  0                                            
        __________________________________________________________________________________________________
        input_2 (InputLayer)            [(None, 28, 28, 3)]  0                                            
        __________________________________________________________________________________________________
        input_3 (InputLayer)            [(None, 28, 28, 3)]  0                                            
        __________________________________________________________________________________________________
        sequential (Sequential)         (None, 126)          1215422     input_1[0][0]                    
                                                                        input_2[0][0]                    
                                                                        input_3[0][0]                    
        __________________________________________________________________________________________________
        concatenate (Concatenate)       (None, 378)          0           sequential[0][0]                 
                                                                        sequential[1][0]                 
                                                                        sequential[2][0]                 
        ==================================================================================================
        @input_shape : (28,28,3) for input which will put in the model
        
        return: model => base model
    '''

    # Define anchor data input
    input_anchor = tf.keras.layers.Input(shape=input_shape)
    # Define positive data input
    input_positive = tf.keras.layers.Input(shape=input_shape)
    # Define negative data input
    input_negative = tf.keras.layers.Input(shape=input_shape)

    embedding_net = create_base_model(input_shape)

    # Use backbone CNN to make embedding model for anchor
    embedding_anchor = embedding_net(input_anchor)
    # Use backbone CNN to make embedding model for positive
    embedding_positive = embedding_net(input_positive)
    # Use backbone CNN to make embedding model for negative
    embedding_negative = embedding_net(input_negative)

    # Merge the three models using concatenate layer
    output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)

    # Make a model with three inputs and concatenated output
    net = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)

    # Show the triplet siamese network model's summary
    net.summary()
    
    return net

def get_result(history, epochs):
    '''
      Get Error rate by epochs and draw a line graph.
      @history : history from training the model
    '''
    # Find trianing accuracy from history of trained model
    accuracy_train = history.history['accuracy']
    # Calculate error
    err_train = [1-x for x in accuracy_train]
    # Find validation accuracy from history of trained model
    val_accuracy = history.history['val_accuracy']
    # Calculate error
    err_val = [1-x for x in val_accuracy]
    # Set x axis' range
    epochs = range(1,epochs+1)
    # Make training error line
    plt.plot(epochs, err_train, 'g', label='Training error')
    # Make validation error line
    plt.plot(epochs, err_val, 'b', label='validation error')
    # Set title
    plt.title('Training and Validation error')
    # Set x label title
    plt.xlabel('Epochs')
    # Set y label title
    plt.ylabel('Error')
    # Make legend
    plt.legend()
    # Show plot
    plt.show()


def make_pairs(x, y, train=True):
      """Creates a tuple containing image pairs with corresponding label.

      Arguments:
          x: List containing images, each index in this list corresponds to one image.
          y: List containing labels, each label with datatype of int.

      Returns:
          Tuple containing two numpy arrays as (pairs_of_samples, labels),
          where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and
          labels are a binary array of shape (2len(x)).
      """
      # Calculate the total number of classes present in the dataset
      num_classes = max(y) + 1
      num_min = min(y)
      # Build the list of idices for images with the same labels
      digit_indices = [np.where(y == i)[0] for i in range(num_classes)] 
      
      # Hold the (image, image) pairs
      pairs = []
      # Indicate whether a pair is positive or negative
      labels = []

      for idx1 in range(len(x)):
          # Add a matching example by getting the current image and label
          x1 = x[idx1]
          label1 = y[idx1]
          
          # Randomly pick an image which belongs to the same class with the current one
          idx2 = random.choice(digit_indices[label1])
          x2 = x[idx2]

          pairs += [[x1, x2]]
          labels += [1]

          # Add a non-matching example
          label2 = random.randint(num_min, num_classes - 1)
          while label2 == label1:
            label2 = random.randint(num_min, num_classes - 1)

          # Randomly pick an image which does not belong to the same class with the current one
          idx2 = random.choice(digit_indices[label2])
          x2 = x[idx2]

          pairs += [[x1, x2]]
          labels += [0]

      return np.array(pairs), np.array(labels).astype("float32")

def euclidean_distance(vects):
      """Find the Euclidean distance between two vectors.

      Arguments:
          vects: List containing two tensors of same length.

      Returns:
          Tensor containing euclidean distance
          (as floating point value) between vectors.
      """
      # Unpack the vectors into separate lists
      x, y = vects
      # Compute the sum of squared distances between the vectors
      sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)
      return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))

def loss(margin=1):
      """Provides 'constrastive_loss' an enclosing scope with variable 'margin'.

    Arguments:
        margin: Integer, defines the baseline for distance for which pairs
                should be classified as dissimilar. - (default is 1).

    Returns:
        'constrastive_loss' function with data ('margin') attached.
    """
      # Contrastive loss = mean( (1-true_value) * square(prediction) +
      #                         true_value * square( max(margin-prediction, 0) ))
      def contrastive_loss(y_true, y_pred):
          """Calculates the constrastive loss.

        Arguments:
            y_true: List of labels, each label is of type float32.
            y_pred: List of predictions of same length as of y_true,
                    each label is of type float32.

        Returns:
            A tensor containing constrastive loss as floating point value.
        """
          # calculate the contrastive loss between the true labels and the predicted labels
          square_pred = tf.math.square(y_pred)
          margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))
          return tf.math.reduce_mean(
              (1 - y_true) * square_pred + (y_true) * margin_square
          )

      return contrastive_loss


def contrastive_model():
  '''
        Create Contrastive Siamese Network
        return: model => contrastive siamese model
    '''
    # Add input layer with shape (28, 28, 3)
  input = layers.Input((28, 28, 3))
  # Add batch normalization layer
  x = tf.keras.layers.BatchNormalization()(input)
  # Add convolutional network, activation is tangent, kernal size is 3*3, outputs are 4
  x = layers.Conv2D(4, (5, 5), activation="tanh")(x)
  # Add average pooling layer, pool size is (2, 2)
  x = layers.AveragePooling2D(pool_size=(2, 2))(x)
  # Add convolutional network, activation is tangent, kernal size is 5*5, outputs are 16
  x = layers.Conv2D(16, (5, 5), activation="tanh")(x)
  # Add average pooling layer, pool size is (2, 2)
  x = layers.AveragePooling2D(pool_size=(2, 2))(x)
  # Add flatten layer
  x = layers.Flatten()(x)

  # Create the contrastive siamese network
  # Add batch normalization layer
  x = tf.keras.layers.BatchNormalization()(x)
  # Add dense layer, activation is tangent, outputs are 10
  x = layers.Dense(10, activation="tanh")(x)

  embedding_network = tf.keras.Model(input, x)

  input_1 = layers.Input((28, 28, 3))
  input_2 = layers.Input((28, 28, 3))

  # As mentioned above, Siamese Network share weights between
  # tower networks (sister networks). To allow this, we will use
  # same embedding network for both tower networks.
  tower_1 = embedding_network(input_1)
  tower_2 = embedding_network(input_2)

  # Merge both tower networks
  merge_layer = layers.Lambda(euclidean_distance)([tower_1, tower_2])
  # Add batch normalization layer
  normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)
  # Add dense layer, activation is sigmoid, output is 1
  output_layer = layers.Dense(1, activation="sigmoid")(normal_layer)
  # Make a model with two inputs and output
  siamese = tf.keras.Model(inputs=[input_1, input_2], outputs=output_layer)

  return siamese

def triplet(X_train,X_val,y_train,y_val,X_test, y_test):
  """
    Experiemnt triplet siamese network
    1. check dataset
    2. generate triplet input dataset
    3. make model
    4. trian model
    5. get result plot
    6. evaluate model

      Arguments:
          X_train: Training images set
          X_val: Validation image set
          X_test: Test image set
          y_train: Training label set
          y_val: Validation label set
          y_test: Test label set
  """
  # Use plot_triplets function to check training dataset
  plot_triplets([X_train[0], X_train[1], X_train[2]])

  # Test generate_triplet_dataset function
  examples = generate_triplet_dataset(X_train, y_train,1)
  # Visualise triplet dataset
  plot_triplets(examples)

  # Set img_size to X_train's length
  img_size = 15424
  # Generate dataset to training
  train_x,train_y = data_generator(X_train,y_train,img_size)
  # Generate dataset to validation
  val_x,val_y = data_generator(X_val,y_val,img_size)

  # Get the model for training 
  train_model = triplet_siamese_network((28,28,3))
  # Compile the triplet siames network with triplet loss
  # Otimizer is adam and it displays accuracy as well
  train_model.compile(loss=triplet_loss, optimizer='adam', metrics=['accuracy'])
  # Train the triplet siamese model
  
  history = train_model.fit(
      train_x,train_y,
      epochs=50, verbose=1,
      batch_size=32,
      validation_data=(val_x,val_y)
  )

  # Get the triplet siamese training results
  get_result(history,50)

  # Generate test dataset for triplet siamese model
  test_x, test_y = data_generator(X_test, y_test, len(X_test))

  # Evaluate with training dataset
  score_training = train_model.evaluate(train_x, train_y)
  print()
  print('Test loss, Test accuracy: ', score_training)

  # Evaluate with all dataset

  # To make all dataset combine training, validation and test images
  X_all = np.concatenate((X_train, X_val))
  X_all = np.concatenate((X_all, X_test))

  # To make all dataset combine training, validation and test labels
  y_all = np.concatenate((y_train, y_val))
  y_all = np.concatenate((y_all, y_test))

  # Generate test dataset
  x, y = data_generator(X_all, y_all, len(X_all))
  # Evaluate with all dataset
  score_all = train_model.evaluate(x,y)
  print()
  print('Test loss, Test accuracy: ', score_all)

  # Test with pairs from the set of glyphs from the test split
  score_test = train_model.evaluate(test_x, test_y)
  print()
  print('Test loss, Test accuracy: ', score_test)

  #--------------------------------------------------------------------------------------#

def contrastive(X_train,X_val,y_train,y_val,X_test, y_test):
    """
      Experiemnt contrastive siamese network
      1. generate contrastive input dataset
      2. make model
      3. trian model
      4. get result plot
      5. evaluate model

        Arguments:
            X_train: Training images set
            X_val: Validation image set
            X_test: Test image set
            y_train: Training label set
            y_val: Validation label set
            y_test: Test label set
    """
    # Specify the number of epochs and batch size
    epochs = 50
    batch_size = 16
    # Margin for contrastive loss
    margin = 1

    # Make train pairs
    pairs_train, labels_train = make_pairs(X_train, y_train)

    # Make validation pairs
    pairs_val, labels_val = make_pairs(X_val, y_val)

    # Make test pairs
    pairs_test, labels_test = make_pairs(X_test, y_test)

    # split dataset
    x_train_1 = pairs_train[:, 0]
    x_train_2 = pairs_train[:, 1]
    # split dataset
    x_val_1 = pairs_val[:, 0]
    x_val_2 = pairs_val[:, 1]
    # split dataset
    x_test_1 = pairs_test[:, 0]
    x_test_2 = pairs_test[:, 1]

    siamese = contrastive_model()

    # Compile the contrastive siamese network with contrastive loss, optimizer is RMSprop with showing accuracy
    siamese.compile(loss=loss(margin=margin), optimizer="RMSprop", metrics=["accuracy"])
    # Show the contrastive siamese network model's summary
    siamese.summary()

    # Train the contrastive siamese model
    constrastive_history = siamese.fit(
        [x_train_1, x_train_2],
        labels_train,
        validation_data=([x_val_1, x_val_2], labels_val),
        batch_size=batch_size,
        epochs=epochs,
    )

    # Get the contrastive siamese training results
    get_result(constrastive_history, epochs)
    # Evaluate with training dataset
    score_training = siamese.evaluate([x_train_1, x_train_2], labels_train)
    print()
    print('Test loss, Test accuracy: ', score_training)

    # Evaluate with all dataset

    # To make all dataset combine training, validation and test images
    X_all = np.concatenate((X_train, X_val))
    X_all = np.concatenate((X_all, X_test))

    # To make all dataset combine training, validation and test labels
    y_all = np.concatenate((y_train, y_val))
    y_all = np.concatenate((y_all, y_test))

    pairs_all, labels_all = make_pairs(X_all, y_all)
    x_all_1 = pairs_all[:, 0]
    x_all_2 = pairs_all[:, 1]

    # Evaluate with all dataset
    score_all = siamese.evaluate([x_all_1,x_all_2],labels_all)
    print()
    print('Test loss, Test accuracy: ', score_all)

    # Test with pairs from the set of glyphs from the test split
    score_test = siamese.evaluate([x_test_1, x_test_2], labels_test)
    print()
    print('Test loss, Test accuracy: ', score_test)

if __name__ == "__main__":
	# Make train dataset using Dataset class
  train = Dataset(training=True)
  # Make validation dataset using Dataset class
  val = Dataset(training=True)
  # Make test dataset using Dataset class
  test = Dataset(training=False)

  # Split trian and valdation datasets since train and val are same dataset.
  # Becasue of the Dataset class, it cannot allow to split train directly.

  # X_train is first 80% data of training dataset and type cast : list to numpy array
  X_train =np.array(train.data[:int(len(train.data)*0.8)])
  # X_val is last 20% data of val dataset and type cast : list to numpy array
  X_val =np.array(val.data[:int(len(train.data)*0.8)])
  # y_train is first 80% data of training dataset and type cast : list to numpy array
  y_train =np.array(train.labels[:int(len(train.data)*0.8)])
  # y_val is first 20% data of val dataset and type cast : list to numpy array
  y_val =np.array(val.labels[:int(len(train.data)*0.8)])

  # Make test dataset and type cast : list to numpy array
  X_test, y_test = np.array(test.data), np.array(test.labels)

  # experiment tirplet siamese network
  triplet(X_train,X_val,y_train,y_val,X_test, y_test)
  # experiment contrastive siamese network
  contrastive(X_train,X_val,y_train,y_val,X_test, y_test)

  # To repeat experiment
  # Set your repeat times and uncommnet next lines
  # repeat = 
  # for i in range(repeat):
  #  triplet(X_train,X_val,y_train,y_val,X_test, y_test)

  # for i in range(repeat):
  #  contrastive(X_train,X_val,y_train,y_val,X_test, y_test)

